#torch vers
torch==2.5.1 
torchvision==0.20.1
torchaudio==2.5.1 
#--index-url https://download.pytorch.org/whl/cu124

#trit deps
nvidia-cuda-runtime-cu12==12.4.127
nvidia-cublas-cu12==12.4.5.8
nvidia-cuda-nvrtc-cu12==12.4.127
nvidia-cuda-nvcc-cu12==12.4.131
nvidia-cufft-cu12==11.2.1.3

#backcompatible cudnn
nvidia-cudnn-cu12==9.1.0.70

#triton win 3.1 tracks cuda 12.4 and pytorch 2.5
#https://github.com/woct0rdho/triton-windows/releases/download/v3.1.0-windows.post8/triton-3.1.0-cp310-cp310-win_amd64.whl

#FA2:
#https://github.com/bdashore3/flash-attention/releases/
#v2.7.1.post1, torch 2.5.1
#https://github.com/bdashore3/flash-attention/releases/download/v2.7.1.post1/flash_attn-2.7.1.post1+cu124torch2.5.1cxx11abiFALSE-cp310-cp310-win_amd64.whl
#... nvm that poster decided to stop compiling backwards()?
#compile it yourself i guess
#last commit before v.2.7.2
#d3b1cd1c1315083578d75f4c90320c501188da20
#073afd5931d6672ff4899429f83a881ff8182fe2
#uv pip install "git+https://github.com/Dao-AILab/flash-attention@073afd5931d6672ff4899429f83a881ff8182fe2"
#uv tool install "git+https://github.com/Dao-AILab/flash-attention@073afd5931d6672ff4899429f83a881ff8182fe2" --no-build-isolation
#nope nope nope. we need flash-attn==2.7.1.post1 exactly bc of cuda 12.4.1 dependency target.
#that isn't posssible? have to use 2.7.2.post1? i don't wnat to think about this.

#...
#uv add flash-attn==2.7.1.post1 --no-build-isolation
#...
#uv add flash-attn==2.7.2.post1 --no-build-isolation