"""
# decode_audio.py
# Decodes token tensors generated by sample_audio_t5.py into audible .wav files.
#
# --- USAGE ---
# uv run decode_audio.py --input_file generated_audio_tokens_your_run.pt
"""
import os
import torch
import torchaudio
import argparse
from encodec import EncodecModel

# --- Configuration via Argparse ---
parser = argparse.ArgumentParser(description="Decode audio tokens from a .pt file into .wav files.")
parser.add_argument('--input_file', type=str, required=True, help="Path to the .pt file generated by the sampling script.")
parser.add_argument('--output_dir', type=str, default='generated_audio', help="Directory to save the output .wav files.")
parser.add_argument('--device', type=str, default='cuda', help="Device to use ('cuda' or 'cpu').")
args = parser.parse_args()

from datetime import datetime
run_id = datetime.now().strftime('%y%m%d%H%M%S') # e.g., "223104" for 10:31:04 PM
print(f"Generated a unique ID for this run's output files: {run_id}")

# --- Main Decoding Logic ---
def decode_tokens_to_wav():
    if not os.path.exists(args.input_file):
        print(f"Error: Input file not found at {args.input_file}")
        return

    # Create output directory if it doesn't exist
    os.makedirs(args.output_dir, exist_ok=True)
    print(f"Output .wav files will be saved in: {args.output_dir}")

    # 1. Load the Encodec model
    print("Loading Encodec 24kHz model...")
    encodec_model = EncodecModel.encodec_model_24khz()
    encodec_model.to(args.device)

    # 2. Load the generated token data
    print(f"Loading token data from: {args.input_file}")
    data = torch.load(args.input_file, map_location=args.device)
    prompts = data['prompts']
    generated = data['generated']
    model_config = data['model_config']
    
    num_samples = prompts.shape[0]
    prompt_length = prompts.shape[1]
    infill_point = prompt_length // 2
    
    print(f"Found {num_samples} samples to decode.")

    # 3. Loop through each sample and decode three versions
    for i in range(num_samples):
        print(f"\n--- Decoding Sample {i+1}/{num_samples} ---")
        
        # A. The original prompt
        prompt_tokens = prompts[i]
        
        # B. The model's generated in-filling/continuation
        generated_tokens = generated[i]
        
        # C. The final combined audio (prompt first half + generated second half)
        combined_tokens = torch.cat([prompt_tokens[:infill_point], generated_tokens])

        # Helper function to decode and save a single tensor
        def save_wav(token_tensor, filename_suffix):
            # Encodec expects shape: [batch, num_codebooks, sequence_length]
            # Our tensor is [sequence_length], so we add the batch and codebook dims.
            codes = token_tensor.unsqueeze(0).unsqueeze(0)
            
            with torch.no_grad():
                # The decode function takes a list of tuples: (codes, scale)
                # For generation, scale is None.
                waveforms = encodec_model.decode([(codes, None)])
            
            # Save the resulting waveform
            filename = os.path.join(args.output_dir, f"{run_id}_sample_{i}_{filename_suffix}.wav")
            # Encodec output is [batch, channels, samples], so we squeeze the batch dim
            torchaudio.save(filename, waveforms.squeeze(0).cpu(), sample_rate=encodec_model.sample_rate)
            print(f"  - Saved {filename}")

        # Decode and save all three versions for easy comparison
        save_wav(prompt_tokens, "original_prompt")
        save_wav(generated_tokens, "generated_infill")
        save_wav(combined_tokens, "combined_final")

    print("\n--- Decoding Complete ---")

if __name__ == '__main__':
    decode_tokens_to_wav()