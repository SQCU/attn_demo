About 20 epochs. 
Context length 512, 
batch size 80 (20 per device over 4 V-100 GPUs), 
16 gradient accumulation steps. 
Learning rate 5e-4, wd=0.1, betas 0.9,0.95. 
The file used to train was indeed 
https://huggingface.co/datasets/roneneldan/TinyStories/blob/main/TinyStories-train.txt.